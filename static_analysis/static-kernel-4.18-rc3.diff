Only in ./linux-4.18-rc3/arch/sh/boot/compressed: vmlinux.scr
Only in ./linux-4.18-rc3/arch/sh/boot/romimage: vmlinux.scr
diff -r -U30 ./linux-4.18-rc3/arch/x86/include/asm/cmpxchg.h kernel_v4.18-rc3/arch/x86/include/asm/cmpxchg.h
--- ./linux-4.18-rc3/arch/x86/include/asm/cmpxchg.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/arch/x86/include/asm/cmpxchg.h	2019-07-28 07:03:45.595084534 +0000
@@ -65,88 +65,88 @@
 			break;						\
 		default:						\
 			__ ## op ## _wrong_size();			\
 		}							\
 		__ret;							\
 	})
 
 /*
  * Note: no "lock" prefix even on SMP: xchg always implies lock anyway.
  * Since this is generally used to protect other memory information, we
  * use "asm volatile" and "memory" clobbers to prevent gcc from moving
  * information around.
  */
 #define xchg(ptr, v)	__xchg_op((ptr), (v), xchg, "")
 
 /*
  * Atomic compare and exchange.  Compare OLD with MEM, if identical,
  * store NEW in MEM.  Return the initial value in MEM.  Success is
  * indicated by comparing RETURN with OLD.
  */
 #define __raw_cmpxchg(ptr, old, new, size, lock)			\
 ({									\
 	__typeof__(*(ptr)) __ret;					\
 	__typeof__(*(ptr)) __old = (old);				\
 	__typeof__(*(ptr)) __new = (new);				\
 	switch (size) {							\
 	case __X86_CASE_B:						\
 	{								\
 		volatile u8 *__ptr = (volatile u8 *)(ptr);		\
 		asm volatile(lock "cmpxchgb %2,%1"			\
-			     : "=a" (__ret), "+m" (*__ptr)		\
-			     : "q" (__new), "0" (__old)			\
+			     : "=m" (__ret), "+m" (*__ptr)		\
+			     : "r" (__new), "0" (__old)			\
 			     : "memory");				\
 		break;							\
 	}								\
 	case __X86_CASE_W:						\
 	{								\
 		volatile u16 *__ptr = (volatile u16 *)(ptr);		\
 		asm volatile(lock "cmpxchgw %2,%1"			\
-			     : "=a" (__ret), "+m" (*__ptr)		\
+			     : "=m" (__ret), "+m" (*__ptr)		\
 			     : "r" (__new), "0" (__old)			\
 			     : "memory");				\
 		break;							\
 	}								\
 	case __X86_CASE_L:						\
 	{								\
 		volatile u32 *__ptr = (volatile u32 *)(ptr);		\
 		asm volatile(lock "cmpxchgl %2,%1"			\
-			     : "=a" (__ret), "+m" (*__ptr)		\
+			     : "=m" (__ret), "+m" (*__ptr)		\
 			     : "r" (__new), "0" (__old)			\
 			     : "memory");				\
 		break;							\
 	}								\
 	case __X86_CASE_Q:						\
 	{								\
 		volatile u64 *__ptr = (volatile u64 *)(ptr);		\
 		asm volatile(lock "cmpxchgq %2,%1"			\
-			     : "=a" (__ret), "+m" (*__ptr)		\
+			     : "=m" (__ret), "+m" (*__ptr)		\
 			     : "r" (__new), "0" (__old)			\
 			     : "memory");				\
 		break;							\
 	}								\
 	default:							\
 		__cmpxchg_wrong_size();					\
 	}								\
 	__ret;								\
 })
 
 #define __cmpxchg(ptr, old, new, size)					\
 	__raw_cmpxchg((ptr), (old), (new), (size), LOCK_PREFIX)
 
 #define __sync_cmpxchg(ptr, old, new, size)				\
 	__raw_cmpxchg((ptr), (old), (new), (size), "lock; ")
 
 #define __cmpxchg_local(ptr, old, new, size)				\
 	__raw_cmpxchg((ptr), (old), (new), (size), "")
 
 #ifdef CONFIG_X86_32
 # include <asm/cmpxchg_32.h>
 #else
 # include <asm/cmpxchg_64.h>
 #endif
 
 #define arch_cmpxchg(ptr, old, new)					\
 	__cmpxchg(ptr, old, new, sizeof(*(ptr)))
 
 #define arch_sync_cmpxchg(ptr, old, new)				\
 	__sync_cmpxchg(ptr, old, new, sizeof(*(ptr)))
diff -r -U30 ./linux-4.18-rc3/arch/x86/include/asm/cpufeature.h kernel_v4.18-rc3/arch/x86/include/asm/cpufeature.h
--- ./linux-4.18-rc3/arch/x86/include/asm/cpufeature.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/arch/x86/include/asm/cpufeature.h	2019-07-28 07:03:45.595084534 +0000
@@ -119,63 +119,65 @@
  * This macro is for detection of features which need kernel
  * infrastructure to be used.  It may *not* directly test the CPU
  * itself.  Use the cpu_has() family if you want true runtime
  * testing of CPU features, like in hypervisor code where you are
  * supporting a possible guest feature where host support for it
  * is not relevant.
  */
 #define cpu_feature_enabled(bit)	\
 	(__builtin_constant_p(bit) && DISABLED_MASK_BIT_SET(bit) ? 0 : static_cpu_has(bit))
 
 #define boot_cpu_has(bit)	cpu_has(&boot_cpu_data, bit)
 
 #define set_cpu_cap(c, bit)	set_bit(bit, (unsigned long *)((c)->x86_capability))
 
 extern void setup_clear_cpu_cap(unsigned int bit);
 extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
 
 #define setup_force_cpu_cap(bit) do { \
 	set_cpu_cap(&boot_cpu_data, bit);	\
 	set_bit(bit, (unsigned long *)cpu_caps_set);	\
 } while (0)
 
 #define setup_force_cpu_bug(bit) setup_force_cpu_cap(bit)
 
 #if defined(__clang__) && !defined(CC_HAVE_ASM_GOTO)
 
 /*
  * Workaround for the sake of BPF compilation which utilizes kernel
  * headers, but clang does not support ASM GOTO and fails the build.
  */
+/*
 #ifndef __BPF_TRACING__
 #warning "Compiler lacks ASM_GOTO support. Add -D __BPF_TRACING__ to your compiler arguments"
 #endif
+*/
 
 #define static_cpu_has(bit)            boot_cpu_has(bit)
 
 #else
 
 /*
  * Static testing of CPU features.  Used the same as boot_cpu_has().
  * These will statically patch the target code for additional
  * performance.
  */
 static __always_inline __pure bool _static_cpu_has(u16 bit)
 {
 	asm_volatile_goto("1: jmp 6f\n"
 		 "2:\n"
 		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
 			 "((5f-4f) - (2b-1b)),0x90\n"
 		 "3:\n"
 		 ".section .altinstructions,\"a\"\n"
 		 " .long 1b - .\n"		/* src offset */
 		 " .long 4f - .\n"		/* repl offset */
 		 " .word %P[always]\n"		/* always replace */
 		 " .byte 3b - 1b\n"		/* src len */
 		 " .byte 5f - 4f\n"		/* repl len */
 		 " .byte 3b - 2b\n"		/* pad len */
 		 ".previous\n"
 		 ".section .altinstr_replacement,\"ax\"\n"
 		 "4: jmp %l[t_no]\n"
 		 "5:\n"
 		 ".previous\n"
 		 ".section .altinstructions,\"a\"\n"
diff -r -U30 ./linux-4.18-rc3/arch/x86/include/asm/current.h kernel_v4.18-rc3/arch/x86/include/asm/current.h
--- ./linux-4.18-rc3/arch/x86/include/asm/current.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/arch/x86/include/asm/current.h	2019-07-28 07:03:45.595084534 +0000
@@ -1,22 +1,22 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_X86_CURRENT_H
 #define _ASM_X86_CURRENT_H
 
 #include <linux/compiler.h>
 #include <asm/percpu.h>
 
 #ifndef __ASSEMBLY__
 struct task_struct;
 
 DECLARE_PER_CPU(struct task_struct *, current_task);
-
+extern struct task_struct init_task;
 static __always_inline struct task_struct *get_current(void)
 {
-	return this_cpu_read_stable(current_task);
+	return &init_task;
 }
 
 #define current get_current()
 
 #endif /* __ASSEMBLY__ */
 
 #endif /* _ASM_X86_CURRENT_H */
diff -r -U30 ./linux-4.18-rc3/arch/x86/Makefile kernel_v4.18-rc3/arch/x86/Makefile
--- ./linux-4.18-rc3/arch/x86/Makefile	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/arch/x86/Makefile	2019-07-28 07:03:45.575084800 +0000
@@ -153,63 +153,63 @@
                 KBUILD_CFLAGS += -DCONFIG_X86_X32_ABI
         else
                 $(warning CONFIG_X86_X32 enabled but no binutils support)
         endif
 endif
 export CONFIG_X86_X32_ABI
 
 #
 # If the function graph tracer is used with mcount instead of fentry,
 # '-maccumulate-outgoing-args' is needed to prevent a GCC bug
 # (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=42109)
 #
 ifdef CONFIG_FUNCTION_GRAPH_TRACER
   ifndef CONFIG_HAVE_FENTRY
 	ACCUMULATE_OUTGOING_ARGS := 1
   else
     ifeq ($(call cc-option-yn, -mfentry), n)
 	ACCUMULATE_OUTGOING_ARGS := 1
 
 	# GCC ignores '-maccumulate-outgoing-args' when used with '-Os'.
 	# If '-Os' is enabled, disable it and print a warning.
         ifdef CONFIG_CC_OPTIMIZE_FOR_SIZE
           undefine CONFIG_CC_OPTIMIZE_FOR_SIZE
           $(warning Disabling CONFIG_CC_OPTIMIZE_FOR_SIZE.  Your compiler does not have -mfentry so you cannot optimize for size with CONFIG_FUNCTION_GRAPH_TRACER.)
         endif
 
     endif
   endif
 endif
 
-ifndef CC_HAVE_ASM_GOTO
-  $(error Compiler lacks asm-goto support.)
-endif
+# ifndef CC_HAVE_ASM_GOTO
+#   $(error Compiler lacks asm-goto support.)
+# endif
 
 #
 # Jump labels need '-maccumulate-outgoing-args' for gcc < 4.5.2 to prevent a
 # GCC bug (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=46226).  There's no way
 # to test for this bug at compile-time because the test case needs to execute,
 # which is a no-go for cross compilers.  So check the GCC version instead.
 #
 ifdef CONFIG_JUMP_LABEL
   ifneq ($(ACCUMULATE_OUTGOING_ARGS), 1)
 	ACCUMULATE_OUTGOING_ARGS = $(call cc-if-fullversion, -lt, 040502, 1)
   endif
 endif
 
 ifeq ($(ACCUMULATE_OUTGOING_ARGS), 1)
 	# This compiler flag is not supported by Clang:
 	KBUILD_CFLAGS += $(call cc-option,-maccumulate-outgoing-args,)
 endif
 
 # Stackpointer is addressed different for 32 bit and 64 bit x86
 sp-$(CONFIG_X86_32) := esp
 sp-$(CONFIG_X86_64) := rsp
 
 # do binutils support CFI?
 cfi := $(call as-instr,.cfi_startproc\n.cfi_rel_offset $(sp-y)$(comma)0\n.cfi_endproc,-DCONFIG_AS_CFI=1)
 # is .cfi_signal_frame supported too?
 cfi-sigframe := $(call as-instr,.cfi_startproc\n.cfi_signal_frame\n.cfi_endproc,-DCONFIG_AS_CFI_SIGNAL_FRAME=1)
 cfi-sections := $(call as-instr,.cfi_sections .debug_frame,-DCONFIG_AS_CFI_SECTIONS=1)
 
 # does binutils support specific instructions?
 asinstr := $(call as-instr,fxsaveq (%rax),-DCONFIG_AS_FXSAVEQ=1)
Only in kernel_v4.18-rc3: build.log
diff -r -U30 ./linux-4.18-rc3/fs/file.c kernel_v4.18-rc3/fs/file.c
--- ./linux-4.18-rc3/fs/file.c	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/fs/file.c	2019-07-28 07:03:48.103051279 +0000
@@ -700,96 +700,96 @@
 }
 
 struct file *fget(unsigned int fd)
 {
 	return __fget(fd, FMODE_PATH);
 }
 EXPORT_SYMBOL(fget);
 
 struct file *fget_raw(unsigned int fd)
 {
 	return __fget(fd, 0);
 }
 EXPORT_SYMBOL(fget_raw);
 
 /*
  * Lightweight file lookup - no refcnt increment if fd table isn't shared.
  *
  * You can use this instead of fget if you satisfy all of the following
  * conditions:
  * 1) You must call fput_light before exiting the syscall and returning control
  *    to userspace (i.e. you cannot remember the returned struct file * after
  *    returning to userspace).
  * 2) You must not call filp_close on the returned struct file * in between
  *    calls to fget_light and fput_light.
  * 3) You must not clone the current task in between the calls to fget_light
  *    and fput_light.
  *
  * The fput_needed flag returned by fget_light should be passed to the
  * corresponding fput_light.
  */
-static unsigned long __fget_light(unsigned int fd, fmode_t mask)
+static struct file* __fget_light(unsigned int fd, fmode_t mask)
 {
 	struct files_struct *files = current->files;
 	struct file *file;
 
 	if (atomic_read(&files->count) == 1) {
 		file = __fcheck_files(files, fd);
 		if (!file || unlikely(file->f_mode & mask))
 			return 0;
-		return (unsigned long)file;
+		return file;//return (unsigned long)file;
 	} else {
 		file = __fget(fd, mask);
 		if (!file)
 			return 0;
-		return FDPUT_FPUT | (unsigned long)file;
+		return file;//return FDPUT_FPUT | (unsigned long)file;
 	}
 }
-unsigned long __fdget(unsigned int fd)
+struct file* __fdget(unsigned int fd)
 {
 	return __fget_light(fd, FMODE_PATH);
 }
 EXPORT_SYMBOL(__fdget);
 
-unsigned long __fdget_raw(unsigned int fd)
+struct file* __fdget_raw(unsigned int fd)
 {
 	return __fget_light(fd, 0);
 }
 
-unsigned long __fdget_pos(unsigned int fd)
+struct file* __fdget_pos(unsigned int fd)
 {
-	unsigned long v = __fdget(fd);
-	struct file *file = (struct file *)(v & ~3);
+	struct file* v = __fdget(fd);
+	struct file *file = v;//(struct file *)(v & ~3);
 
 	if (file && (file->f_mode & FMODE_ATOMIC_POS)) {
 		if (file_count(file) > 1) {
-			v |= FDPUT_POS_UNLOCK;
+			//v |= FDPUT_POS_UNLOCK;
 			mutex_lock(&file->f_pos_lock);
 		}
 	}
 	return v;
 }
 
 void __f_unlock_pos(struct file *f)
 {
 	mutex_unlock(&f->f_pos_lock);
 }
 
 /*
  * We only lock f_pos if we have threads or if the file might be
  * shared with another process. In both cases we'll have an elevated
  * file count (done either by fdget() or by fork()).
  */
 
 void set_close_on_exec(unsigned int fd, int flag)
 {
 	struct files_struct *files = current->files;
 	struct fdtable *fdt;
 	spin_lock(&files->file_lock);
 	fdt = files_fdtable(files);
 	if (flag)
 		__set_close_on_exec(fd, fdt);
 	else
 		__clear_close_on_exec(fd, fdt);
 	spin_unlock(&files->file_lock);
 }
 
Only in ./linux-4.18-rc3/: .get_maintainer.ignore
Only in ./linux-4.18-rc3/: .gitattributes
diff -r -U30 ./linux-4.18-rc3/include/linux/compiler.h kernel_v4.18-rc3/include/linux/compiler.h
--- ./linux-4.18-rc3/include/linux/compiler.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/include/linux/compiler.h	2019-07-28 07:03:48.255049264 +0000
@@ -228,81 +228,81 @@
  * put the two invocations of READ_ONCE or WRITE_ONCE in different C
  * statements.
  *
  * These two macros will also work on aggregate data types like structs or
  * unions. If the size of the accessed data type exceeds the word size of
  * the machine (e.g., 32 bits or 64 bits) READ_ONCE() and WRITE_ONCE() will
  * fall back to memcpy(). There's at least two memcpy()s: one for the
  * __builtin_memcpy() and then one for the macro doing the copy of variable
  * - '__u' allocated on the stack.
  *
  * Their two major use cases are: (1) Mediating communication between
  * process-level code and irq/NMI handlers, all running on the same CPU,
  * and (2) Ensuring that the compiler does not fold, spindle, or otherwise
  * mutilate accesses that either do not require ordering or that interact
  * with an explicit memory barrier or atomic instruction that provides the
  * required ordering.
  */
 #include <asm/barrier.h>
 #include <linux/kasan-checks.h>
 
 #define __READ_ONCE(x, check)						\
 ({									\
 	union { typeof(x) __val; char __c[1]; } __u;			\
 	if (check)							\
 		__read_once_size(&(x), __u.__c, sizeof(x));		\
 	else								\
 		__read_once_size_nocheck(&(x), __u.__c, sizeof(x));	\
 	smp_read_barrier_depends(); /* Enforce dependency ordering from x */ \
 	__u.__val;							\
 })
-#define READ_ONCE(x) __READ_ONCE(x, 1)
+#define READ_ONCE(x) x//__READ_ONCE(x, 1)
 
 /*
  * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need
  * to hide memory access from KASAN.
  */
-#define READ_ONCE_NOCHECK(x) __READ_ONCE(x, 0)
+#define READ_ONCE_NOCHECK(x) x//__READ_ONCE(x, 0)
 
 static __no_kasan_or_inline
 unsigned long read_word_at_a_time(const void *addr)
 {
 	kasan_check_read(addr, 1);
 	return *(unsigned long *)addr;
 }
 
 #define WRITE_ONCE(x, val) \
 ({							\
-	union { typeof(x) __val; char __c[1]; } __u =	\
-		{ .__val = (__force typeof(x)) (val) }; \
-	__write_once_size(&(x), __u.__c, sizeof(x));	\
-	__u.__val;					\
+	/*union { typeof(x) __val; char __c[1]; } __u =	*/\
+		/*{ .__val = (__force typeof(x)) (val) }; */\
+	x = val;/*__write_once_size(&(x), __u.__c, sizeof(x));	*/\
+	val;/*__u.__val;					*/\
 })
 
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */
 
 #ifndef __optimize
 # define __optimize(level)
 #endif
 
 /* Compile time object size, -1 for unknown */
 #ifndef __compiletime_object_size
 # define __compiletime_object_size(obj) -1
 #endif
 #ifndef __compiletime_warning
 # define __compiletime_warning(message)
 #endif
 #ifndef __compiletime_error
 # define __compiletime_error(message)
 /*
  * Sparse complains of variable sized arrays due to the temporary variable in
  * __compiletime_assert. Unfortunately we can't just expand it out to make
  * sparse see a constant array size without breaking compiletime_assert on old
  * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
  */
 # ifndef __CHECKER__
 #  define __compiletime_error_fallback(condition) \
 	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
 # endif
 #endif
diff -r -U30 ./linux-4.18-rc3/include/linux/file.h kernel_v4.18-rc3/include/linux/file.h
--- ./linux-4.18-rc3/include/linux/file.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/include/linux/file.h	2019-07-28 07:03:48.263049158 +0000
@@ -15,68 +15,68 @@
 extern void fput(struct file *);
 
 struct file_operations;
 struct vfsmount;
 struct dentry;
 struct path;
 extern struct file *alloc_file(const struct path *, fmode_t mode,
 	const struct file_operations *fop);
 
 static inline void fput_light(struct file *file, int fput_needed)
 {
 	if (fput_needed)
 		fput(file);
 }
 
 struct fd {
 	struct file *file;
 	unsigned int flags;
 };
 #define FDPUT_FPUT       1
 #define FDPUT_POS_UNLOCK 2
 
 static inline void fdput(struct fd fd)
 {
 	if (fd.flags & FDPUT_FPUT)
 		fput(fd.file);
 }
 
 extern struct file *fget(unsigned int fd);
 extern struct file *fget_raw(unsigned int fd);
-extern unsigned long __fdget(unsigned int fd);
-extern unsigned long __fdget_raw(unsigned int fd);
-extern unsigned long __fdget_pos(unsigned int fd);
+extern struct file* __fdget(unsigned int fd);
+extern struct file* __fdget_raw(unsigned int fd);
+extern struct file* __fdget_pos(unsigned int fd);
 extern void __f_unlock_pos(struct file *);
 
-static inline struct fd __to_fd(unsigned long v)
+static inline struct fd __to_fd(struct file* v)
 {
-	return (struct fd){(struct file *)(v & ~3),v & 3};
+	return (struct fd){(struct file *)v, (unsigned long)(v) & 3};
 }
 
 static inline struct fd fdget(unsigned int fd)
 {
 	return __to_fd(__fdget(fd));
 }
 
 static inline struct fd fdget_raw(unsigned int fd)
 {
 	return __to_fd(__fdget_raw(fd));
 }
 
 static inline struct fd fdget_pos(int fd)
 {
 	return __to_fd(__fdget_pos(fd));
 }
 
 static inline void fdput_pos(struct fd f)
 {
 	if (f.flags & FDPUT_POS_UNLOCK)
 		__f_unlock_pos(f.file);
 	fdput(f);
 }
 
 extern int f_dupfd(unsigned int from, struct file *file, unsigned flags);
 extern int replace_fd(unsigned fd, struct file *file, unsigned flags);
 extern void set_close_on_exec(unsigned int fd, int flag);
 extern bool get_close_on_exec(unsigned int fd);
 extern void put_filp(struct file *);
 extern int get_unused_fd_flags(unsigned flags);
diff -r -U30 ./linux-4.18-rc3/include/linux/rcupdate.h kernel_v4.18-rc3/include/linux/rcupdate.h
--- ./linux-4.18-rc3/include/linux/rcupdate.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/include/linux/rcupdate.h	2019-07-28 07:03:48.323048362 +0000
@@ -377,215 +377,215 @@
  * @v: value to assign (publish)
  *
  * Assigns the specified value to the specified RCU-protected
  * pointer, ensuring that any concurrent RCU readers will see
  * any prior initialization.
  *
  * Inserts memory barriers on architectures that require them
  * (which is most of them), and also prevents the compiler from
  * reordering the code that initializes the structure after the pointer
  * assignment.  More importantly, this call documents which pointers
  * will be dereferenced by RCU read-side code.
  *
  * In some special cases, you may use RCU_INIT_POINTER() instead
  * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
  * to the fact that it does not constrain either the CPU or the compiler.
  * That said, using RCU_INIT_POINTER() when you should have used
  * rcu_assign_pointer() is a very bad thing that results in
  * impossible-to-diagnose memory corruption.  So please be careful.
  * See the RCU_INIT_POINTER() comment header for details.
  *
  * Note that rcu_assign_pointer() evaluates each of its arguments only
  * once, appearances notwithstanding.  One of the "extra" evaluations
  * is in typeof() and the other visible only to sparse (__CHECKER__),
  * neither of which actually execute the argument.  As with most cpp
  * macros, this execute-arguments-only-once property is important, so
  * please be careful when making changes to rcu_assign_pointer() and the
  * other macros that it invokes.
  */
 #define rcu_assign_pointer(p, v)					      \
 ({									      \
-	uintptr_t _r_a_p__v = (uintptr_t)(v);				      \
+	p = v;/*uintptr_t _r_a_p__v = (uintptr_t)(v);				      */\
 									      \
-	if (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)	      \
-		WRITE_ONCE((p), (typeof(p))(_r_a_p__v));		      \
-	else								      \
-		smp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \
-	_r_a_p__v;							      \
+	/*if (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)	      */\
+		/*WRITE_ONCE((p), (typeof(p))(_r_a_p__v));		      */\
+	/*else								      */\
+		/*smp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); */\
+	/*_r_a_p__v;							      */\
 })
 
 /**
  * rcu_swap_protected() - swap an RCU and a regular pointer
  * @rcu_ptr: RCU pointer
  * @ptr: regular pointer
  * @c: the conditions under which the dereference will take place
  *
  * Perform swap(@rcu_ptr, @ptr) where @rcu_ptr is an RCU-annotated pointer and
  * @c is the argument that is passed to the rcu_dereference_protected() call
  * used to read that pointer.
  */
 #define rcu_swap_protected(rcu_ptr, ptr, c) do {			\
 	typeof(ptr) __tmp = rcu_dereference_protected((rcu_ptr), (c));	\
 	rcu_assign_pointer((rcu_ptr), (ptr));				\
 	(ptr) = __tmp;							\
 } while (0)
 
 /**
  * rcu_access_pointer() - fetch RCU pointer with no dereferencing
  * @p: The pointer to read
  *
  * Return the value of the specified RCU-protected pointer, but omit the
  * lockdep checks for being in an RCU read-side critical section.  This is
  * useful when the value of this pointer is accessed, but the pointer is
  * not dereferenced, for example, when testing an RCU-protected pointer
  * against NULL.  Although rcu_access_pointer() may also be used in cases
  * where update-side locks prevent the value of the pointer from changing,
  * you should instead use rcu_dereference_protected() for this use case.
  *
  * It is also permissible to use rcu_access_pointer() when read-side
  * access to the pointer was removed at least one grace period ago, as
  * is the case in the context of the RCU callback that is freeing up
  * the data, or after a synchronize_rcu() returns.  This can be useful
  * when tearing down multi-linked structures after a grace period
  * has elapsed.
  */
-#define rcu_access_pointer(p) __rcu_access_pointer((p), __rcu)
+#define rcu_access_pointer(p) p //__rcu_access_pointer((p), __rcu)
 
 /**
  * rcu_dereference_check() - rcu_dereference with debug checking
  * @p: The pointer to read, prior to dereferencing
  * @c: The conditions under which the dereference will take place
  *
  * Do an rcu_dereference(), but check that the conditions under which the
  * dereference will take place are correct.  Typically the conditions
  * indicate the various locking conditions that should be held at that
  * point.  The check should return true if the conditions are satisfied.
  * An implicit check for being in an RCU read-side critical section
  * (rcu_read_lock()) is included.
  *
  * For example:
  *
  *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock));
  *
  * could be used to indicate to lockdep that foo->bar may only be dereferenced
  * if either rcu_read_lock() is held, or that the lock required to replace
  * the bar struct at foo->bar is held.
  *
  * Note that the list of conditions may also include indications of when a lock
  * need not be held, for example during initialisation or destruction of the
  * target struct:
  *
  *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock) ||
  *					      atomic_read(&foo->usage) == 0);
  *
  * Inserts memory barriers on architectures that require them
  * (currently only the Alpha), prevents the compiler from refetching
  * (and from merging fetches), and, more importantly, documents exactly
  * which pointers are protected by RCU and checks that the pointer is
  * annotated as __rcu.
  */
 #define rcu_dereference_check(p, c) \
-	__rcu_dereference_check((p), (c) || rcu_read_lock_held(), __rcu)
+	p//__rcu_dereference_check((p), (c) || rcu_read_lock_held(), __rcu)
 
 /**
  * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking
  * @p: The pointer to read, prior to dereferencing
  * @c: The conditions under which the dereference will take place
  *
  * This is the RCU-bh counterpart to rcu_dereference_check().
  */
 #define rcu_dereference_bh_check(p, c) \
-	__rcu_dereference_check((p), (c) || rcu_read_lock_bh_held(), __rcu)
+	p//__rcu_dereference_check((p), (c) || rcu_read_lock_bh_held(), __rcu)
 
 /**
  * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking
  * @p: The pointer to read, prior to dereferencing
  * @c: The conditions under which the dereference will take place
  *
  * This is the RCU-sched counterpart to rcu_dereference_check().
  */
 #define rcu_dereference_sched_check(p, c) \
-	__rcu_dereference_check((p), (c) || rcu_read_lock_sched_held(), \
-				__rcu)
+	p/*__rcu_dereference_check((p), (c) || rcu_read_lock_sched_held(), \
+				__rcu)*/
 
 /*
  * The tracing infrastructure traces RCU (we want that), but unfortunately
  * some of the RCU checks causes tracing to lock up the system.
  *
  * The no-tracing version of rcu_dereference_raw() must not call
  * rcu_read_lock_held().
  */
-#define rcu_dereference_raw_notrace(p) __rcu_dereference_check((p), 1, __rcu)
+#define rcu_dereference_raw_notrace(p) p //__rcu_dereference_check((p), 1, __rcu)
 
 /**
  * rcu_dereference_protected() - fetch RCU pointer when updates prevented
  * @p: The pointer to read, prior to dereferencing
  * @c: The conditions under which the dereference will take place
  *
  * Return the value of the specified RCU-protected pointer, but omit
  * the READ_ONCE().  This is useful in cases where update-side locks
  * prevent the value of the pointer from changing.  Please note that this
  * primitive does *not* prevent the compiler from repeating this reference
  * or combining it with other references, so it should not be used without
  * protection of appropriate locks.
  *
  * This function is only for update-side use.  Using this function
  * when protected only by rcu_read_lock() will result in infrequent
  * but very ugly failures.
  */
 #define rcu_dereference_protected(p, c) \
-	__rcu_dereference_protected((p), (c), __rcu)
+	p //__rcu_dereference_protected((p), (c), __rcu)
 
 
 /**
  * rcu_dereference() - fetch RCU-protected pointer for dereferencing
  * @p: The pointer to read, prior to dereferencing
  *
  * This is a simple wrapper around rcu_dereference_check().
  */
-#define rcu_dereference(p) rcu_dereference_check(p, 0)
+#define rcu_dereference(p) p //rcu_dereference_check(p, 0)
 
 /**
  * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing
  * @p: The pointer to read, prior to dereferencing
  *
  * Makes rcu_dereference_check() do the dirty work.
  */
-#define rcu_dereference_bh(p) rcu_dereference_bh_check(p, 0)
+#define rcu_dereference_bh(p) p //rcu_dereference_bh_check(p, 0)
 
 /**
  * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing
  * @p: The pointer to read, prior to dereferencing
  *
  * Makes rcu_dereference_check() do the dirty work.
  */
-#define rcu_dereference_sched(p) rcu_dereference_sched_check(p, 0)
+#define rcu_dereference_sched(p) p //rcu_dereference_sched_check(p, 0)
 
 /**
  * rcu_pointer_handoff() - Hand off a pointer from RCU to other mechanism
  * @p: The pointer to hand off
  *
  * This is simply an identity function, but it documents where a pointer
  * is handed off from RCU to some other synchronization mechanism, for
  * example, reference counting or locking.  In C11, it would map to
  * kill_dependency().  It could be used as follows:
  * ``
  *	rcu_read_lock();
  *	p = rcu_dereference(gp);
  *	long_lived = is_long_lived(p);
  *	if (long_lived) {
  *		if (!atomic_inc_not_zero(p->refcnt))
  *			long_lived = false;
  *		else
  *			p = rcu_pointer_handoff(p);
  *	}
  *	rcu_read_unlock();
  *``
  */
 #define rcu_pointer_handoff(p) (p)
 
 /**
  * rcu_read_lock() - mark the beginning of an RCU read-side critical section
  *
  * When synchronize_rcu() is invoked on one CPU while other CPUs
  * are within RCU read-side critical sections, then the
  * synchronize_rcu() is guaranteed to block until after all the other
@@ -844,47 +844,47 @@
 		kfree_call_rcu(head, (rcu_callback_t)(unsigned long)(offset)); \
 	} while (0)
 
 /**
  * kfree_rcu() - kfree an object after a grace period.
  * @ptr:	pointer to kfree
  * @rcu_head:	the name of the struct rcu_head within the type of @ptr.
  *
  * Many rcu callbacks functions just call kfree() on the base structure.
  * These functions are trivial, but their size adds up, and furthermore
  * when they are used in a kernel module, that module must invoke the
  * high-latency rcu_barrier() function at module-unload time.
  *
  * The kfree_rcu() function handles this issue.  Rather than encoding a
  * function address in the embedded rcu_head structure, kfree_rcu() instead
  * encodes the offset of the rcu_head structure within the base structure.
  * Because the functions are not allowed in the low-order 4096 bytes of
  * kernel virtual memory, offsets up to 4095 bytes can be accommodated.
  * If the offset is larger than 4095 bytes, a compile-time error will
  * be generated in __kfree_rcu().  If this error is triggered, you can
  * either fall back to use of call_rcu() or rearrange the structure to
  * position the rcu_head structure into the first 4096 bytes.
  *
  * Note that the allowable offset might decrease in the future, for example,
  * to allow something like kmem_cache_free_rcu().
  *
  * The BUILD_BUG_ON check must not involve any function calls, hence the
  * checks are done in macros here.
  */
 #define kfree_rcu(ptr, rcu_head)					\
-	__kfree_rcu(&((ptr)->rcu_head), offsetof(typeof(*(ptr)), rcu_head))
+	kfree(ptr)//__kfree_rcu(&((ptr)->rcu_head), offsetof(typeof(*(ptr)), rcu_head))
 
 
 /*
  * Place this after a lock-acquisition primitive to guarantee that
  * an UNLOCK+LOCK pair acts as a full barrier.  This guarantee applies
  * if the UNLOCK and LOCK are executed by the same CPU or if the
  * UNLOCK and LOCK operate on the same lock variable.
  */
 #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE
 #define smp_mb__after_unlock_lock()	smp_mb()  /* Full ordering for lock. */
 #else /* #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE */
 #define smp_mb__after_unlock_lock()	do { } while (0)
 #endif /* #else #ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE */
 
 
 #endif /* __LINUX_RCUPDATE_H */
diff -r -U30 ./linux-4.18-rc3/include/linux/slab.h kernel_v4.18-rc3/include/linux/slab.h
--- ./linux-4.18-rc3/include/linux/slab.h	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/include/linux/slab.h	2019-07-28 07:03:48.331048256 +0000
@@ -467,84 +467,84 @@
  * %GFP_HIGHUSER - Allocate pages from high memory.
  *
  * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
  *
  * %GFP_NOFS - Do not make any fs calls while trying to get memory.
  *
  * %GFP_NOWAIT - Allocation will not sleep.
  *
  * %__GFP_THISNODE - Allocate node-local memory only.
  *
  * %GFP_DMA - Allocation suitable for DMA.
  *   Should only be used for kmalloc() caches. Otherwise, use a
  *   slab created with SLAB_DMA.
  *
  * Also it is possible to set different flags by OR'ing
  * in one or more of the following additional @flags:
  *
  * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
  *
  * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
  *   (think twice before using).
  *
  * %__GFP_NORETRY - If memory is not immediately available,
  *   then give up at once.
  *
  * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
  *
  * %__GFP_RETRY_MAYFAIL - Try really hard to succeed the allocation but fail
  *   eventually.
  *
- * There are other flags available as well, but these are not intended
- * for general use, and so are not documented here. For a full list of
- * potential flags, always refer to linux/gfp.h.
- */
+ * There are other flags available as well, but these are not intended*/
+#if defined(__clang__)
+extern void *kmalloc(size_t size, gfp_t flags);
+#else
 static __always_inline void *kmalloc(size_t size, gfp_t flags)
 {
 	if (__builtin_constant_p(size)) {
 		if (size > KMALLOC_MAX_CACHE_SIZE)
 			return kmalloc_large(size, flags);
 #ifndef CONFIG_SLOB
 		if (!(flags & GFP_DMA)) {
 			unsigned int index = kmalloc_index(size);
 
 			if (!index)
 				return ZERO_SIZE_PTR;
 
 			return kmem_cache_alloc_trace(kmalloc_caches[index],
 					flags, size);
 		}
 #endif
 	}
 	return __kmalloc(size, flags);
 }
-
+#endif
 /*
  * Determine size used for the nth kmalloc cache.
  * return size or 0 if a kmalloc cache for that
  * size does not exist
  */
 static __always_inline unsigned int kmalloc_size(unsigned int n)
 {
 #ifndef CONFIG_SLOB
 	if (n > 2)
 		return 1U << n;
 
 	if (n == 1 && KMALLOC_MIN_SIZE <= 32)
 		return 96;
 
 	if (n == 2 && KMALLOC_MIN_SIZE <= 64)
 		return 192;
 #endif
 	return 0;
 }
 
 static __always_inline void *kmalloc_node(size_t size, gfp_t flags, int node)
 {
 #ifndef CONFIG_SLOB
 	if (__builtin_constant_p(size) &&
 		size <= KMALLOC_MAX_CACHE_SIZE && !(flags & GFP_DMA)) {
 		unsigned int i = kmalloc_index(size);
 
 		if (!i)
 			return ZERO_SIZE_PTR;
 
diff -r -U30 ./linux-4.18-rc3/init/init_task.c kernel_v4.18-rc3/init/init_task.c
--- ./linux-4.18-rc3/init/init_task.c	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/init/init_task.c	2019-07-28 07:03:48.443046771 +0000
@@ -39,61 +39,61 @@
 static struct sighand_struct init_sighand = {
 	.count		= ATOMIC_INIT(1),
 	.action		= { { { .sa_handler = SIG_DFL, } }, },
 	.siglock	= __SPIN_LOCK_UNLOCKED(init_sighand.siglock),
 	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(init_sighand.signalfd_wqh),
 };
 
 /*
  * Set up the first task table, touch at your own risk!. Base=0,
  * limit=0x1fffff (=2MB)
  */
 struct task_struct init_task
 #ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK
 	__init_task_data
 #endif
 = {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	.thread_info	= INIT_THREAD_INFO(init_task),
 	.stack_refcount	= ATOMIC_INIT(1),
 #endif
 	.state		= 0,
 	.stack		= init_stack,
 	.usage		= ATOMIC_INIT(2),
 	.flags		= PF_KTHREAD,
 	.prio		= MAX_PRIO - 20,
 	.static_prio	= MAX_PRIO - 20,
 	.normal_prio	= MAX_PRIO - 20,
 	.policy		= SCHED_NORMAL,
 	.cpus_allowed	= CPU_MASK_ALL,
 	.nr_cpus_allowed= NR_CPUS,
-	.mm		= NULL,
+	.mm		= &init_mm,
 	.active_mm	= &init_mm,
 	.restart_block	= {
 		.fn = do_no_restart_syscall,
 	},
 	.se		= {
 		.group_node 	= LIST_HEAD_INIT(init_task.se.group_node),
 	},
 	.rt		= {
 		.run_list	= LIST_HEAD_INIT(init_task.rt.run_list),
 		.time_slice	= RR_TIMESLICE,
 	},
 	.tasks		= LIST_HEAD_INIT(init_task.tasks),
 #ifdef CONFIG_SMP
 	.pushable_tasks	= PLIST_NODE_INIT(init_task.pushable_tasks, MAX_PRIO),
 #endif
 #ifdef CONFIG_CGROUP_SCHED
 	.sched_task_group = &root_task_group,
 #endif
 	.ptraced	= LIST_HEAD_INIT(init_task.ptraced),
 	.ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
 	.real_parent	= &init_task,
 	.parent		= &init_task,
 	.children	= LIST_HEAD_INIT(init_task.children),
 	.sibling	= LIST_HEAD_INIT(init_task.sibling),
 	.group_leader	= &init_task,
 	RCU_POINTER_INITIALIZER(real_cred, &init_cred),
 	RCU_POINTER_INITIALIZER(cred, &init_cred),
 	.comm		= INIT_TASK_COMM,
 	.thread		= INIT_THREAD,
 	.fs		= &init_fs,
diff -r -U30 ./linux-4.18-rc3/Makefile kernel_v4.18-rc3/Makefile
--- ./linux-4.18-rc3/Makefile	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/Makefile	2019-07-28 07:03:44.943093179 +0000
@@ -474,65 +474,65 @@
 outputmakefile:
 ifneq ($(KBUILD_SRC),)
 	$(Q)ln -fsn $(srctree) source
 	$(Q)$(CONFIG_SHELL) $(srctree)/scripts/mkmakefile \
 	    $(srctree) $(objtree) $(VERSION) $(PATCHLEVEL)
 endif
 
 ifeq ($(cc-name),clang)
 ifneq ($(CROSS_COMPILE),)
 CLANG_TARGET	:= --target=$(notdir $(CROSS_COMPILE:%-=%))
 GCC_TOOLCHAIN	:= $(realpath $(dir $(shell which $(LD)))/..)
 endif
 ifneq ($(GCC_TOOLCHAIN),)
 CLANG_GCC_TC	:= --gcc-toolchain=$(GCC_TOOLCHAIN)
 endif
 KBUILD_CFLAGS += $(CLANG_TARGET) $(CLANG_GCC_TC)
 KBUILD_AFLAGS += $(CLANG_TARGET) $(CLANG_GCC_TC)
 KBUILD_CFLAGS += $(call cc-option, -no-integrated-as)
 KBUILD_AFLAGS += $(call cc-option, -no-integrated-as)
 endif
 
 RETPOLINE_CFLAGS_GCC := -mindirect-branch=thunk-extern -mindirect-branch-register
 RETPOLINE_CFLAGS_CLANG := -mretpoline-external-thunk
 RETPOLINE_CFLAGS := $(call cc-option,$(RETPOLINE_CFLAGS_GCC),$(call cc-option,$(RETPOLINE_CFLAGS_CLANG)))
 export RETPOLINE_CFLAGS
 
 KBUILD_CFLAGS	+= $(call cc-option,-fno-PIE)
 KBUILD_AFLAGS	+= $(call cc-option,-fno-PIE)
 
 # check for 'asm goto'
-ifeq ($(shell $(CONFIG_SHELL) $(srctree)/scripts/gcc-goto.sh $(CC) $(KBUILD_CFLAGS)), y)
-  CC_HAVE_ASM_GOTO := 1
-  KBUILD_CFLAGS += -DCC_HAVE_ASM_GOTO
-  KBUILD_AFLAGS += -DCC_HAVE_ASM_GOTO
-endif
+# ifeq ($(shell $(CONFIG_SHELL) $(srctree)/scripts/gcc-goto.sh $(CC) $(KBUILD_CFLAGS)), y)
+#   CC_HAVE_ASM_GOTO := 1
+#   KBUILD_CFLAGS += -DCC_HAVE_ASM_GOTO
+#   KBUILD_AFLAGS += -DCC_HAVE_ASM_GOTO
+# endif
 
 ifeq ($(shell $(CONFIG_SHELL) $(srctree)/scripts/cc-can-link.sh $(CC)), y)
   CC_CAN_LINK := y
   export CC_CAN_LINK
 endif
 
 # The expansion should be delayed until arch/$(SRCARCH)/Makefile is included.
 # Some architectures define CROSS_COMPILE in arch/$(SRCARCH)/Makefile.
 # CC_VERSION_TEXT is referenced from Kconfig (so it needs export),
 # and from include/config/auto.conf.cmd to detect the compiler upgrade.
 CC_VERSION_TEXT = $(shell $(CC) --version | head -n 1)
 
 ifeq ($(config-targets),1)
 # ===========================================================================
 # *config targets only - make sure prerequisites are updated, and descend
 # in scripts/kconfig to make the *config target
 
 # Read arch specific Makefile to set KBUILD_DEFCONFIG as needed.
 # KBUILD_DEFCONFIG may point out an alternative default configuration
 # used for 'make defconfig'
 include arch/$(SRCARCH)/Makefile
 export KBUILD_DEFCONFIG KBUILD_KCONFIG CC_VERSION_TEXT
 
 config: scripts_basic outputmakefile FORCE
 	$(Q)$(MAKE) $(build)=scripts/kconfig $@
 
 %config: scripts_basic outputmakefile FORCE
 	$(Q)$(MAKE) $(build)=scripts/kconfig $@
 
 else
diff -r -U30 ./linux-4.18-rc3/mm/init-mm.c kernel_v4.18-rc3/mm/init-mm.c
--- ./linux-4.18-rc3/mm/init-mm.c	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/mm/init-mm.c	2019-07-28 07:03:48.511045869 +0000
@@ -1,29 +1,29 @@
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/mm_types.h>
 #include <linux/rbtree.h>
 #include <linux/rwsem.h>
 #include <linux/spinlock.h>
 #include <linux/list.h>
 #include <linux/cpumask.h>
 
 #include <linux/atomic.h>
 #include <linux/user_namespace.h>
 #include <asm/pgtable.h>
 #include <asm/mmu.h>
 
 #ifndef INIT_MM_CONTEXT
 #define INIT_MM_CONTEXT(name)
 #endif
-
+pgd_t temp_pgd;
 struct mm_struct init_mm = {
 	.mm_rb		= RB_ROOT,
-	.pgd		= swapper_pg_dir,
+	.pgd		= &temp_pgd,
 	.mm_users	= ATOMIC_INIT(2),
 	.mm_count	= ATOMIC_INIT(1),
 	.mmap_sem	= __RWSEM_INITIALIZER(init_mm.mmap_sem),
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.arg_lock	=  __SPIN_LOCK_UNLOCKED(init_mm.arg_lock),
 	.mmlist		= LIST_HEAD_INIT(init_mm.mmlist),
 	.user_ns	= &init_user_ns,
 	INIT_MM_CONTEXT(init_mm)
 };
diff -r -U30 ./linux-4.18-rc3/mm/slab.c kernel_v4.18-rc3/mm/slab.c
--- ./linux-4.18-rc3/mm/slab.c	2018-07-01 23:04:53.000000000 +0000
+++ kernel_v4.18-rc3/mm/slab.c	2019-07-28 07:03:48.523045710 +0000
@@ -3516,76 +3516,76 @@
 	 * is per page memory  reference) to get nodeid. Instead use a global
 	 * variable to skip the call, which is mostly likely to be present in
 	 * the cache.
 	 */
 	if (nr_online_nodes > 1 && cache_free_alien(cachep, objp))
 		return;
 
 	if (ac->avail < ac->limit) {
 		STATS_INC_FREEHIT(cachep);
 	} else {
 		STATS_INC_FREEMISS(cachep);
 		cache_flusharray(cachep, ac);
 	}
 
 	if (sk_memalloc_socks()) {
 		struct page *page = virt_to_head_page(objp);
 
 		if (unlikely(PageSlabPfmemalloc(page))) {
 			cache_free_pfmemalloc(cachep, page, objp);
 			return;
 		}
 	}
 
 	ac->entry[ac->avail++] = objp;
 }
 
 /**
  * kmem_cache_alloc - Allocate an object
  * @cachep: The cache to allocate from.
  * @flags: See kmalloc().
- *
- * Allocate an object from this cache.  The flags are only relevant
- * if the cache has no available objects.
  */
+#if defined(__clang__)
+extern void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags);
+#else
 void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
 {
 	void *ret = slab_alloc(cachep, flags, _RET_IP_);
 
 	kasan_slab_alloc(cachep, ret, flags);
 	trace_kmem_cache_alloc(_RET_IP_, ret,
 			       cachep->object_size, cachep->size, flags);
 
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc);
-
+#endif
 static __always_inline void
 cache_alloc_debugcheck_after_bulk(struct kmem_cache *s, gfp_t flags,
 				  size_t size, void **p, unsigned long caller)
 {
 	size_t i;
 
 	for (i = 0; i < size; i++)
 		p[i] = cache_alloc_debugcheck_after(s, flags, p[i], caller);
 }
 
 int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 			  void **p)
 {
 	size_t i;
 
 	s = slab_pre_alloc_hook(s, flags);
 	if (!s)
 		return 0;
 
 	cache_alloc_debugcheck_before(s, flags);
 
 	local_irq_disable();
 	for (i = 0; i < size; i++) {
 		void *objp = __do_cache_alloc(s, flags);
 
 		if (unlikely(!objp))
 			goto error;
 		p[i] = objp;
 	}
 	local_irq_enable();
@@ -3763,85 +3763,85 @@
 void kmem_cache_free_bulk(struct kmem_cache *orig_s, size_t size, void **p)
 {
 	struct kmem_cache *s;
 	size_t i;
 
 	local_irq_disable();
 	for (i = 0; i < size; i++) {
 		void *objp = p[i];
 
 		if (!orig_s) /* called via kfree_bulk */
 			s = virt_to_cache(objp);
 		else
 			s = cache_from_obj(orig_s, objp);
 
 		debug_check_no_locks_freed(objp, s->object_size);
 		if (!(s->flags & SLAB_DEBUG_OBJECTS))
 			debug_check_no_obj_freed(objp, s->object_size);
 
 		__cache_free(s, objp, _RET_IP_);
 	}
 	local_irq_enable();
 
 	/* FIXME: add tracing */
 }
 EXPORT_SYMBOL(kmem_cache_free_bulk);
 
 /**
  * kfree - free previously allocated memory
  * @objp: pointer returned by kmalloc.
  *
- * If @objp is NULL, no operation is performed.
- *
- * Don't free memory not originally allocated by kmalloc()
- * or you will run into trouble.
- */
+ * If @objp is NULL, no operation is performed.*/
+
+#if defined(__clang__)
+extern void kfree(const void *objp);
+#else
 void kfree(const void *objp)
 {
 	struct kmem_cache *c;
 	unsigned long flags;
 
 	trace_kfree(_RET_IP_, objp);
 
 	if (unlikely(ZERO_OR_NULL_PTR(objp)))
 		return;
 	local_irq_save(flags);
 	kfree_debugcheck(objp);
 	c = virt_to_cache(objp);
 	debug_check_no_locks_freed(objp, c->object_size);
 
 	debug_check_no_obj_freed(objp, c->object_size);
 	__cache_free(c, (void *)objp, _RET_IP_);
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(kfree);
-
+#endif
 /*
  * This initializes kmem_cache_node or resizes various caches for all nodes.
  */
 static int setup_kmem_cache_nodes(struct kmem_cache *cachep, gfp_t gfp)
 {
 	int ret;
 	int node;
 	struct kmem_cache_node *n;
 
 	for_each_online_node(node) {
 		ret = setup_kmem_cache_node(cachep, node, gfp, true);
 		if (ret)
 			goto fail;
 
 	}
 
 	return 0;
 
 fail:
 	if (!cachep->list.next) {
 		/* Cache is not active yet. Roll back what we did */
 		node--;
 		while (node >= 0) {
 			n = get_node(cachep, node);
 			if (n) {
 				kfree(n->shared);
 				free_alien_cache(n->alien);
 				kfree(n);
 				cachep->node[node] = NULL;
 			}
Only in kernel_v4.18-rc3: patch
Only in kernel_v4.18-rc3/scripts/basic: fixdep
Only in kernel_v4.18-rc3/scripts/basic: .fixdep.cmd
Only in kernel_v4.18-rc3/scripts/kconfig: conf
Only in kernel_v4.18-rc3/scripts/kconfig: .conf.cmd
Only in kernel_v4.18-rc3/scripts/kconfig: conf.o
Only in kernel_v4.18-rc3/scripts/kconfig: .conf.o.cmd
Only in kernel_v4.18-rc3/scripts/kconfig: zconf.lex.c
Only in kernel_v4.18-rc3/scripts/kconfig: .zconf.lex.c.cmd
Only in kernel_v4.18-rc3/scripts/kconfig: zconf.tab.c
Only in kernel_v4.18-rc3/scripts/kconfig: .zconf.tab.c.cmd
Only in kernel_v4.18-rc3/scripts/kconfig: zconf.tab.o
Only in kernel_v4.18-rc3/scripts/kconfig: .zconf.tab.o.cmd
Only in ./linux-4.18-rc3/tools/bpf/bpftool: bash-completion
Only in ./linux-4.18-rc3/tools/power/acpi: tools
